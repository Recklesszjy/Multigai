{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Distribution, Normal, constraints, kl_divergence, Bernoulli, NegativeBinomial\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse\n",
    "from torch.optim import Adam\n",
    "import pickle\n",
    "import anndata as ad\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set Python built-in random seed\n",
    "    random.seed(seed)  \n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed) \n",
    "    \n",
    "    # Set PyTorch CPU random seed\n",
    "    torch.manual_seed(seed) \n",
    "    \n",
    "    # Set PyTorch GPU random seed (current device)\n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    \n",
    "    # Set PyTorch GPU random seed (all devices)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    \n",
    "    # Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    \n",
    "    # Disable CuDNN auto-tuner to guarantee reproducibility\n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "class ZeroInflatedNegativeBinomial(Distribution):\n",
    "    \"\"\"\n",
    "    Zero-Inflated Negative Binomial (ZINB) distribution.\n",
    "\n",
    "    This distribution is commonly used to model over-dispersed count data\n",
    "    with excessive zeros, such as scRNA-seq gene expression counts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constraints on distribution parameters\n",
    "    arg_constraints = {\n",
    "        \"mu\": constraints.greater_than_eq(0),      # Mean of the NB distribution\n",
    "        \"theta\": constraints.greater_than_eq(0),   # Inverse dispersion parameter\n",
    "        \"zi_logits\": constraints.real,              # Logits for zero-inflation probability\n",
    "        \"scale\": constraints.greater_than_eq(0),   # Optional scaling factor (e.g. library size)\n",
    "    }\n",
    "\n",
    "    # Support of the distribution: non-negative integers\n",
    "    support = constraints.nonnegative_integer\n",
    "\n",
    "    def __init__(self, mu, theta, zi_logits, scale, eps=1e-8, validate_args=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : torch.Tensor\n",
    "            Mean of the Negative Binomial distribution.\n",
    "        theta : torch.Tensor\n",
    "            Inverse dispersion parameter of the NB distribution.\n",
    "        zi_logits : torch.Tensor\n",
    "            Logits controlling the zero-inflation probability.\n",
    "        scale : torch.Tensor\n",
    "            Scaling factor applied to the mean (e.g. size factor).\n",
    "        eps : float\n",
    "            Small constant for numerical stability.\n",
    "        validate_args : bool\n",
    "            Whether to validate distribution arguments.\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.zi_logits = zi_logits\n",
    "        self.scale = scale \n",
    "        self.eps = eps\n",
    "\n",
    "        # Initialize base Distribution class\n",
    "        super().__init__(validate_args=validate_args)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Compute log-probability of observed counts under ZINB.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Observed count data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Log-likelihood of each observation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert zero-inflation logits to probability\n",
    "        pi = torch.sigmoid(self.zi_logits)\n",
    "\n",
    "        # Log-probability under the Negative Binomial distribution\n",
    "        log_nb = (\n",
    "            torch.lgamma(x + self.theta)\n",
    "            - torch.lgamma(self.theta)\n",
    "            - torch.lgamma(x + 1)\n",
    "            + self.theta * torch.log(self.theta + self.eps)\n",
    "            + x * torch.log(self.mu + self.eps)\n",
    "            - (x + self.theta) * torch.log(self.mu + self.theta + self.eps)\n",
    "        )\n",
    "\n",
    "        # Zero-inflated mixture:\n",
    "        # - If x == 0: mixture of structural zero and NB zero\n",
    "        # - If x > 0: NB probability scaled by (1 - pi)\n",
    "        log_prob = torch.where(\n",
    "            (x == 0),\n",
    "            torch.log(pi + (1 - pi) * torch.exp(log_nb) + self.eps),\n",
    "            torch.log(1 - pi + self.eps) + log_nb,\n",
    "        )\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "class multigain(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, input_dim3, n_hidden, hidden, z_dim, batch_dim, q_dim=128, kv_n=64, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Multi-modal VAE model supporting three input modalities (m1, m2, m3) \n",
    "        and modeling outputs with Zero-Inflated Negative Binomial (ZINB) distributions.\n",
    "        \n",
    "        Args:\n",
    "            input_dim1, input_dim2, input_dim3: Input dimensions for the three modalities\n",
    "            n_hidden: Number of hidden layers in encoder/decoder\n",
    "            hidden: Hidden layer dimension\n",
    "            z_dim: Latent variable dimension\n",
    "            batch_dim: One-hot batch encoding dimension\n",
    "            q_dim: Query vector dimension (for attention, not currently used)\n",
    "            kv_n: Number of key/value vectors (for attention, not currently used)\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kv_n = kv_n\n",
    "        self.q_dim = q_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.batch_dim = batch_dim\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # ===== Encoder builder =====\n",
    "        # make_encoder constructs n_hidden fully connected layers with LayerNorm + ReLU + Dropout\n",
    "        def make_encoder(in_dim):\n",
    "            layers = []\n",
    "            for _ in range(n_hidden):\n",
    "                layers.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(in_dim, hidden),\n",
    "                        nn.LayerNorm(hidden),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout_rate)\n",
    "                    )\n",
    "                )\n",
    "                in_dim = hidden\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # Three encoders for three modalities\n",
    "        self.encoder1 = make_encoder(input_dim1)\n",
    "        self.encoder2 = make_encoder(input_dim2)\n",
    "        self.encoder3 = make_encoder(input_dim3)\n",
    "\n",
    "        # ===== Latent variable networks =====\n",
    "        # m_net outputs mean (mu)\n",
    "        # l_net outputs log-variance (logvar)\n",
    "        self.m_net = nn.Linear(hidden, z_dim)\n",
    "        self.l_net = nn.Linear(hidden, z_dim)\n",
    "\n",
    "        # ===== Decoder =====\n",
    "        # Shared base layers + modality-specific heads\n",
    "        self.decoder_base = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(z_dim + batch_dim if i == 0 else hidden + batch_dim, hidden),\n",
    "                nn.LayerNorm(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ) for i in range(n_hidden)\n",
    "        ])\n",
    "\n",
    "        # ZINB output heads for three modalities\n",
    "        self.fc_scale1 = nn.Sequential(nn.Linear(hidden + batch_dim, input_dim1), nn.Softmax(dim=-1))\n",
    "        self.fc_dropout1 = nn.Linear(hidden + batch_dim, input_dim1)\n",
    "        self.fc_r1 = nn.Parameter(torch.randn(input_dim1))  # Learnable dispersion\n",
    "\n",
    "        self.fc_scale2 = nn.Sequential(nn.Linear(hidden + batch_dim, input_dim2), nn.Softmax(dim=-1))\n",
    "        self.fc_dropout2 = nn.Linear(hidden + batch_dim, input_dim2)\n",
    "        self.fc_r2 = nn.Parameter(torch.randn(input_dim2))\n",
    "\n",
    "        self.fc_scale3 = nn.Sequential(nn.Linear(hidden + batch_dim, input_dim3), nn.Softmax(dim=-1))\n",
    "        self.fc_dropout3 = nn.Linear(hidden + batch_dim, input_dim3)\n",
    "        self.fc_r3 = nn.Parameter(torch.randn(input_dim3))\n",
    "\n",
    "    def decode_from_z(self, dz, m1, m2, m3, m, batch):\n",
    "        \"\"\"\n",
    "        Decode latent variable z into ZINB distributions for each modality.\n",
    "\n",
    "        Args:\n",
    "            dz: Latent variable z\n",
    "            m1, m2, m3: Original modality inputs (for library size calculation)\n",
    "            m: Modality type, 12 -> m1+m2, 13 -> m1+m3\n",
    "            batch: One-hot batch encoding\n",
    "\n",
    "        Returns:\n",
    "            p1, p2, p3: ZINB distribution objects for each modality\n",
    "        \"\"\"\n",
    "        for layer in self.decoder_base:\n",
    "            dz = torch.cat([dz, batch], dim=1)  # Concatenate batch info\n",
    "            dz = layer(dz)\n",
    "        final = torch.cat([dz, batch], dim=1)\n",
    "\n",
    "        p1 = p2 = p3 = None\n",
    "        if m in [12, 13]:\n",
    "            # Decode modality 1\n",
    "            scale1 = self.fc_scale1(final)\n",
    "            dropout1 = self.fc_dropout1(final)\n",
    "            library1 = torch.log(m1.sum(1, keepdim=True) + 1e-8)\n",
    "            rate1 = torch.exp(library1) * scale1\n",
    "            p1 = ZeroInflatedNegativeBinomial(mu=rate1, theta=torch.exp(self.fc_r1), zi_logits=dropout1, scale=scale1)\n",
    "\n",
    "            # Decode modality 2\n",
    "            if m == 12:\n",
    "                scale2 = self.fc_scale2(final)\n",
    "                dropout2 = self.fc_dropout2(final)\n",
    "                library2 = torch.log(m2.sum(1, keepdim=True) + 1e-8)\n",
    "                rate2 = torch.exp(library2) * scale2\n",
    "                p2 = ZeroInflatedNegativeBinomial(mu=rate2, theta=torch.exp(self.fc_r2), zi_logits=dropout2, scale=scale2)\n",
    "\n",
    "            # Decode modality 3\n",
    "            if m == 13:\n",
    "                scale3 = self.fc_scale3(final)\n",
    "                dropout3 = self.fc_dropout3(final)\n",
    "                library3 = torch.log(m3.sum(1, keepdim=True) + 1e-8)\n",
    "                rate3 = torch.exp(library3) * scale3\n",
    "                p3 = ZeroInflatedNegativeBinomial(mu=rate3, theta=torch.exp(self.fc_r3), zi_logits=dropout3, scale=scale3)\n",
    "\n",
    "        return p1, p2, p3\n",
    "\n",
    "    def forward(self, m1, m2, m3, m, batch):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            m1, m2, m3: Modality inputs\n",
    "            m: Modality type, 12 -> m1+m2, 13 -> m1+m3\n",
    "            batch: One-hot batch encoding\n",
    "\n",
    "        Returns:\n",
    "            z: Sampled latent variable\n",
    "            p1, p2, p3: Decoded ZINB distributions\n",
    "            qz: Posterior distribution\n",
    "            pz: Prior distribution\n",
    "            a1_all, a2_all, a3_all: Encoder hidden representations\n",
    "        \"\"\"\n",
    "        device = batch.device\n",
    "        batch_size = m1.size(0)\n",
    "\n",
    "        # Initialize placeholders\n",
    "        mu_all = torch.zeros(batch_size, self.z_dim, device=device)\n",
    "        var_all = torch.zeros(batch_size, self.z_dim, device=device)\n",
    "        a1_all = torch.zeros(batch_size, self.hidden, device=device)\n",
    "        a2_all = torch.zeros(batch_size, self.hidden, device=device)\n",
    "        a3_all = torch.zeros(batch_size, self.hidden, device=device)\n",
    "\n",
    "        # ===== Encode =====\n",
    "        if m in [12, 13]:\n",
    "            e1 = self.encoder1(m1)\n",
    "            if m == 12:\n",
    "                e2 = self.encoder2(m2)\n",
    "                ae = (e1 + e2) / 2.0  # Simple averaging\n",
    "            if m == 13:\n",
    "                e3 = self.encoder3(m3)\n",
    "                ae = (e1 + e3) / 2.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported modality m={m}\")\n",
    "\n",
    "        # Latent variable mean and variance\n",
    "        mu = self.m_net(ae)\n",
    "        logvar = self.l_net(ae)\n",
    "        var = torch.exp(logvar) + 1e-8\n",
    "\n",
    "        mu_all = mu\n",
    "        var_all = var\n",
    "        a1_all = e1\n",
    "        a2_all = e2 if m == 12 else None\n",
    "        a3_all = e3 if m == 13 else None\n",
    "\n",
    "        var_all = torch.clamp(var_all, min=1e-6)  # Prevent numerical instability\n",
    "        qz = Normal(mu_all, var_all.sqrt())  # Posterior\n",
    "        z = qz.rsample()  # Reparameterization trick\n",
    "        pz = Normal(torch.zeros_like(z), torch.ones_like(z))  # Prior\n",
    "\n",
    "        # Decode\n",
    "        p1, p2, p3 = self.decode_from_z(z, m1, m2, m3, m, batch)\n",
    "        return z, p1, p2, p3, qz, pz, a1_all, a2_all, a3_all\n",
    "\n",
    "    def loss_function(self, m1, m2, m3, m, p1, p2, p3, q, p, a1, a2, a3, w):\n",
    "        \"\"\"\n",
    "        Compute total loss: reconstruction + KL divergence + cosine similarity loss\n",
    "\n",
    "        Args:\n",
    "            p1,p2,p3: ZINB distribution objects\n",
    "            q, p: Posterior and prior distributions\n",
    "            a1,a2,a3: Encoder hidden representations\n",
    "            w: KL weight\n",
    "\n",
    "        Returns:\n",
    "            loss: Total loss\n",
    "            reconst_loss: Reconstruction loss\n",
    "            kl: KL divergence\n",
    "            cos_loss: Cosine alignment loss\n",
    "        \"\"\"\n",
    "        device = m1.device\n",
    "        cos_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        if m == 12:\n",
    "            # Modality 1 + 2\n",
    "            reconst_loss1 = -p1.log_prob(m1).sum(-1).mean()\n",
    "            reconst_loss2 = -p2.log_prob(m2).sum(-1).mean()\n",
    "            reconst_loss = reconst_loss1 + reconst_loss2\n",
    "            cos_sim = F.cosine_similarity(a1, a2, dim=1)\n",
    "            cos_loss = (1 - cos_sim).mean()  # Alignment loss\n",
    "        elif m == 13:\n",
    "            # Modality 1 + 3\n",
    "            reconst_loss1 = -p1.log_prob(m1).sum(-1).mean()\n",
    "            reconst_loss3 = -p3.log_prob(m3).sum(-1).mean()\n",
    "            reconst_loss = reconst_loss1 + reconst_loss3\n",
    "            cos_sim = F.cosine_similarity(a1, a3, dim=1)\n",
    "            cos_loss = (1 - cos_sim).mean()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported modality m={m}\")\n",
    "\n",
    "        kl = torch.distributions.kl_divergence(q, p).sum(dim=-1).mean()  # KL divergence\n",
    "        loss = reconst_loss + w * kl + cos_loss\n",
    "        return loss, reconst_loss, kl, cos_loss\n",
    "    \n",
    "class MultiOmicsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for multi-omics data.\n",
    "\n",
    "    Each sample contains:\n",
    "    - Three modality feature vectors (m1, m2, m3)\n",
    "    - A modality indicator m\n",
    "    - A batch covariate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.m1_data = args[0]     # Modality 1 data (e.g., scRNA-seq)\n",
    "        self.m2_data = args[1]     # Modality 2 data (e.g., scATAC-seq)\n",
    "        self.m3_data = args[2]     # Modality 3 data (e.g., ADT)\n",
    "        self.m_data = args[3]      # Modality indicator (12 or 13)\n",
    "        self.batch_data = args[4]  # Batch labels / covariates\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of samples\n",
    "        return len(self.batch_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert each modality to float tensor\n",
    "        m1 = torch.tensor(self.m1_data[idx], dtype=torch.float32).squeeze(0)\n",
    "        m2 = torch.tensor(self.m2_data[idx], dtype=torch.float32).squeeze(0)\n",
    "        m3 = torch.tensor(self.m3_data[idx], dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        # Modality combination indicator\n",
    "        m = torch.tensor(self.m_data[idx], dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        # Batch covariate\n",
    "        batch = self.batch_data[idx]\n",
    "\n",
    "        return m1, m2, m3, m, batch, idx\n",
    "    \n",
    "def train_and_evaluate_model(output_path,\n",
    "                             train_loader, test_loader, adata,\n",
    "                             *args,\n",
    "                             num_epochs=200):\n",
    "    \"\"\"\n",
    "    Train the multigain model and extract latent representations for the dataset.\n",
    "\n",
    "    Args:\n",
    "        output_path: File path to save the AnnData object with latent space\n",
    "        train_loader: PyTorch DataLoader for training batches\n",
    "        test_loader: PyTorch DataLoader for test/evaluation batches\n",
    "        adata: AnnData object containing the dataset\n",
    "        *args: Model hyperparameters in order:\n",
    "               input_dim1, input_dim2, input_dim3, n_hidden, hidden, z_dim, batch_dim, q_dim, kv_n\n",
    "        num_epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Unpack model hyperparameters\n",
    "    input_dim1, input_dim2, input_dim3, n_hidden, hidden, z_dim, batch_dim, q_dim, kv_n = args\n",
    "\n",
    "    # Initialize the model and move to GPU/CPU\n",
    "    model = multigain(input_dim1, input_dim2, input_dim3,\n",
    "                     n_hidden, hidden, z_dim,\n",
    "                     batch_dim, q_dim, kv_n).to(device)\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer_main = Adam(model.parameters(), lr=0.001)\n",
    "    scheduler_main = torch.optim.lr_scheduler.StepLR(optimizer_main, step_size=50, gamma=0.9)\n",
    "\n",
    "    tqdm_bar = tqdm(range(num_epochs), desc=\"Training Progress\")\n",
    "\n",
    "    # ===== Training Loop =====\n",
    "    for epoch in tqdm_bar:\n",
    "        running_loss = 0.0\n",
    "        running_recon = 0.0\n",
    "        running_kl = 0.0\n",
    "        running_cos = 0.0\n",
    "\n",
    "        # KL weight schedule: 0 for first 100 epochs, then 0.1\n",
    "        kl_weight = 0.0 if epoch < 100 else 0.1\n",
    "\n",
    "        model.train()\n",
    "        for batch_data in train_loader:\n",
    "            optimizer_main.zero_grad()\n",
    "\n",
    "            m_values = batch_data[3]  # modality identifiers for each sample\n",
    "            unique_m = m_values.unique()  # get unique modalities in this batch\n",
    "\n",
    "            # Shuffle modalities to randomize training order\n",
    "            perm = torch.randperm(len(unique_m))\n",
    "            unique_m = unique_m[perm]\n",
    "\n",
    "            # Process each modality separately\n",
    "            for m_curr in unique_m:\n",
    "                mask = (m_values == m_curr)  # select samples with current modality\n",
    "\n",
    "                if mask.any():\n",
    "                    # Select sub-batch corresponding to this modality\n",
    "                    sub_batch = [d[mask] for d in batch_data]\n",
    "                    m1, m2, m3, m_tensor, batch_tensor, idx = [x.to(device) for x in sub_batch]\n",
    "\n",
    "                    # Forward pass\n",
    "                    z, p1, p2, p3, qz, pz, a1, a2, a3 = model(\n",
    "                        m1, m2, m3,\n",
    "                        int(m_curr.item()),\n",
    "                        batch_tensor\n",
    "                    )\n",
    "\n",
    "                    # Compute loss: reconstruction + KL + cosine similarity\n",
    "                    loss, reconst_loss, kl_loss, cos_loss = model.loss_function(\n",
    "                        m1, m2, m3,\n",
    "                        int(m_curr.item()),\n",
    "                        p1, p2, p3, qz, pz,\n",
    "                        a1, a2, a3,\n",
    "                        kl_weight\n",
    "                    )\n",
    "\n",
    "                    # Backpropagation\n",
    "                    loss.backward()\n",
    "                    optimizer_main.step()\n",
    "\n",
    "                    # Accumulate losses for reporting\n",
    "                    running_loss += loss.item()\n",
    "                    running_recon += reconst_loss.item()\n",
    "                    running_kl += kl_loss.item()\n",
    "                    running_cos += cos_loss.item()\n",
    "\n",
    "        # Average losses per batch\n",
    "        n_batches = len(train_loader)\n",
    "        tqdm_bar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/n_batches:.4f}\",\n",
    "            \"recon\": f\"{running_recon/n_batches:.4f}\",\n",
    "            \"kl\": f\"{running_kl/n_batches:.4f}\",\n",
    "            \"cos\": f\"{running_cos/n_batches:.4f}\",\n",
    "            \"w\": f\"{kl_weight:.3f}\"\n",
    "        })\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler_main.step()\n",
    "\n",
    "    # ===== Evaluation =====\n",
    "    model.eval()\n",
    "    z_all = torch.zeros((len(adata), z_dim), device=device)  # placeholder for latent vectors\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            indices = batch_data[-1]  # sample indices in the original dataset\n",
    "            m_values = batch_data[3]\n",
    "            unique_m = m_values.unique()\n",
    "\n",
    "            for m_curr in unique_m:\n",
    "                mask = (m_values == m_curr)\n",
    "                if mask.any():\n",
    "                    # Sub-batch for this modality\n",
    "                    sub_batch = [d[mask] for d in batch_data]\n",
    "                    m1, m2, m3, m_tensor, batch_tensor, idx = [x.to(device) for x in sub_batch]\n",
    "\n",
    "                    # Forward pass to extract latent variable z\n",
    "                    z, _, _, _, _, _, _, _, _ = model(\n",
    "                        m1, m2, m3, int(m_curr.item()), batch_tensor\n",
    "                    )\n",
    "\n",
    "                    # Store latent vectors at correct indices\n",
    "                    z_all[idx.long()] = z\n",
    "\n",
    "    # Save latent vectors to AnnData object\n",
    "    adata.obsm['latent'] = z_all.cpu().numpy()\n",
    "    adata.write_h5ad(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = sc.read(\"./data/neurips-multiome/rna_hvg.h5ad\") \n",
    "atac = sc.read(\"./data/neurips-multiome/atac_hvf.h5ad\") \n",
    "rna_d, atac_d = rna.layers['counts'], atac.layers['counts'] \n",
    "adt_d = np.zeros((rna_d.shape[0], 1)) \n",
    "rna_dim, atac_dim, adt_dim = rna.shape[1], atac.shape[1], 1 \n",
    "if issparse(rna_d): rna_d = rna_d.toarray() \n",
    "if issparse(atac_d): atac_d = atac_d.toarray() \n",
    "modality_map = { 'multiome': 12, 'cite': 13} \n",
    "modality_vector = rna.obs['Modality'].map(modality_map) \n",
    "modality_d = modality_vector.to_numpy().astype(float) \n",
    "batch_indices = torch.from_numpy(rna.obs['batch'].astype('category').cat.codes.values).long() \n",
    "batch_encoded = torch.nn.functional.one_hot(batch_indices) \n",
    "batch_dim = batch_encoded.shape[1] \n",
    "dataset = MultiOmicsDataset(rna_d, atac_d, adt_d, modality_d, batch_encoded) \n",
    "train_loader = DataLoader(dataset, batch_size=512, shuffle=True) \n",
    "test_loader = DataLoader(dataset, batch_size=512, shuffle=False) \n",
    "train_and_evaluate_model('./results/neurips-multiome-multigain.h5ad', train_loader, test_loader, rna, rna_dim, atac_dim, adt_dim, 1, 128, 30, batch_dim, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = sc.read(\"./data/neurips-cite/rna_hvg.h5ad\") \n",
    "adt = sc.read(\"./data/neurips-cite/protein.h5ad\") \n",
    "rna_d, adt_d = rna.layers['counts'], adt.layers['counts'] \n",
    "atac_d = np.zeros((rna_d.shape[0], 1))  \n",
    "rna_dim, atac_dim, adt_dim = rna.shape[1], 1, adt.shape[1] \n",
    "if issparse(rna_d): rna_d = rna_d.toarray() \n",
    "if issparse(adt_d): adt_d = adt_d.toarray() \n",
    "modality_map = { 'multiome': 12, 'cite': 13} \n",
    "modality_vector = rna.obs['Modality'].map(modality_map) \n",
    "modality_d = modality_vector.to_numpy().astype(float) \n",
    "batch_indices = torch.from_numpy(rna.obs['batch'].astype('category').cat.codes.values).long() \n",
    "batch_encoded = torch.nn.functional.one_hot(batch_indices) \n",
    "batch_dim = batch_encoded.shape[1] \n",
    "dataset = MultiOmicsDataset(rna_d, atac_d, adt_d, modality_d, batch_encoded) \n",
    "train_loader = DataLoader(dataset, batch_size=512, shuffle=True) \n",
    "test_loader = DataLoader(dataset, batch_size=512, shuffle=False) \n",
    "train_and_evaluate_model('./results/neurips-cite-multigain.h5ad', train_loader, test_loader, rna, rna_dim, atac_dim, adt_dim, 1, 128, 30, batch_dim, 128, 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multisi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
